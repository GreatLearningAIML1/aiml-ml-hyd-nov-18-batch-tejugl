{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SeqNLP_Project1_Questions-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "\n",
        "### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "\n",
        "It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n",
        "\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "`from keras.datasets import imdb`\n",
        "\n",
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "\n",
        "### Aim\n",
        "\n",
        "1. Import test and train data  \n",
        "2. Import the labels ( train and test) \n",
        "3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n",
        "4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n",
        "5. Report the Accuracy of the model. (5 points)  \n",
        "6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "#### Usage:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwAkEFhUX0ZV",
        "colab_type": "text"
      },
      "source": [
        "### Handle the below error using below snippet\n",
        "\"Object arrays cannot be loaded when allow_pickle=False\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmF2XXSSUVdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np_load_old = np.load\n",
        "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui_yfkA1XuhM",
        "colab_type": "text"
      },
      "source": [
        "###1. Import test and train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxvR1grRUa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JrXFSFtRUbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9wPYVRvb9Bg",
        "colab_type": "text"
      },
      "source": [
        "##Description of the data\n",
        "(From https://keras.io/datasets/ --> \"IMDB Movie reviews sentiment classification\")\n",
        "\n",
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRhaMcTFRUbC",
        "colab_type": "code",
        "outputId": "ce153cd3-d0ff-4638-a374-e6b63c0556e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#visualizing the data\n",
        "print ('review: ', x_train[0], 'label: ', y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1   14\n",
            "   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173\n",
            "   36  256    5   25  100   43  838  112   50  670    2    9   35  480\n",
            "  284    5  150    4  172  112  167    2  336  385   39    4  172 4536\n",
            " 1111   17  546   38   13  447    4  192   50   16    6  147 2025   19\n",
            "   14   22    4 1920 4613  469    4   22   71   87   12   16   43  530\n",
            "   38   76   15   13 1247    4   22   17  515   17   12   16  626   18\n",
            "    2    5   62  386   12    8  316    8  106    5    4 2223 5244   16\n",
            "  480   66 3785   33    4  130   12   16   38  619    5   25  124   51\n",
            "   36  135   48   25 1415   33    6   22   12  215   28   77   52    5\n",
            "   14  407   16   82    2    8    4  107  117 5952   15  256    4    2\n",
            "    7 3766    5  723   36   71   43  530  476   26  400  317   46    7\n",
            "    4    2 1029   13  104   88    4  381   15  297   98   32 2071   56\n",
            "   26  141    6  194 7486   18    4  226   22   21  134  476   26  480\n",
            "    5  144   30 5535   18   51   36   28  224   92   25  104    4  226\n",
            "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
            "   15   16 5345   19  178   32] label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdV_C-c-RUbF",
        "colab_type": "code",
        "outputId": "eb68cca6-5448-4de1-dc95-6779ffc133b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 300) (25000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN5fEV0qXk7U",
        "colab_type": "text"
      },
      "source": [
        "###3. Get the word index and then Create key value pair for word and word_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTgGAgEnVRjU",
        "colab_type": "code",
        "outputId": "5d60781b-6a42-4b4b-9a87-0f9027a90e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "word_id = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQln8EBchAe",
        "colab_type": "code",
        "outputId": "d736eef1-032d-474f-c12c-4dea415703db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "id_to_word = {i: word for word, i in word_id.items()}\n",
        "print('review text')\n",
        "print([id_to_word.get(i, ' ') for i in x_train[4]])\n",
        "print('label')\n",
        "print(y_train[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review text\n",
            "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'the', 'sure', 'themes', 'br', 'only', 'acting', 'i', 'i', 'was', 'favourite', 'as', 'on', 'she', 'they', 'hat', 'but', 'already', 'most', 'was', 'scares', 'minor', 'if', 'flash', 'was', 'well', 'also', 'good', '8', 'older', 'was', 'with', 'enjoy', 'used', 'enjoy', 'phone', 'too', \"i'm\", 'of', 'you', 'an', 'job', 'br', 'only', 'women', 'than', 'robot', 'to', 'was', 'with', 'these', 'unexpected', 'sure', 'little', 'sure', 'guy', 'sure', 'on', 'was', 'one', 'your', 'life', 'was', 'children', 'in', 'particularly', 'only', 'yes', 'she', 'sort', 'is', 'jerry', 'but', 'so', 'stories', 'them', 'final', 'known', 'to', 'have', 'does', 'such', 'most', 'that', 'supposed', 'imagination', 'very', 'moving', 'antonioni', 'only', 'yes', 'this', 'was', 'seconds', 'for', 'imagination', 'on', 'this', 'of', 'and', 'to', 'plays', 'that', 'nights', 'to', 'for', 'supposed', 'still', 'been', 'last', 'fan', 'always', 'your', 'bit', 'that', 'strong', 'said', 'clean', 'knowing', 'br', 'theory', 'to', 'car', 'masterpiece', 'out', 'in', 'also', 'show', 'for', \"film's\", 'was', 'tale', 'have', 'flash', 'but', 'look', 'part', \"i'm\", 'film', 'as', 'to', 'penelope', 'is', 'script', 'hard', 'br', 'only', 'acting']\n",
            "label\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k_vPe4ndFhe",
        "colab_type": "code",
        "outputId": "687f3a35-9986-48c3-d051-e92d54254fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#Test data\n",
        "print('review text')\n",
        "print([id_to_word.get(i, ' ') for i in x_test[4]])\n",
        "print('label')\n",
        "print(y_test[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review text\n",
            "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'the', 'just', 'good', 'because', 'great', 'cold', 'watching', 'is', 'minute', 'each', 'shirley', 'completely', 'to', 'was', 'several', 'as', 'b', 'i', 'i', 'as', 'b', 'gave', 'compared', 'rest', 'not', 'includes', 'we', 'if', 'main', 'that', 'movie', 'sometimes', 'movie', 'have', 'sex', 'man', 'endearing', 'of', 'feet', 'he', 'played', 'to', 'and', 'from', 'into', 'pot', 'have', 'and', 'man', 'second', 'hand', 'in', 'and', 'watching', 'his', 'offering', 'as', 'b', 'it', 'other', 'and', 'to', 'it', 'taste', 'bit', 'i', 'i', 'in', 'perfect', 'as', 'slowly', 'truth', 'was', 'one', 'in', 'perfect', 'only', 'deliver', 'sleazy', 'has', 'thrown', 'not', 'wonder', 'classic', 'as', 'b', 'satisfied', 'at', 'main', 'that', 'i', 'i', 'their', 'among', 'among', 'without', \"didn't\", 'later', 'if', 'for', 'very', 'and', \"didn't\", 'clearly', 'and', \"didn't\", 'forget', \"didn't\"]\n",
            "label\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjrloSXYdza8",
        "colab_type": "text"
      },
      "source": [
        "### 4. Build a Sequential Model using Keras for Sentiment Classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ki9t3ORRUbH",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTwWKVwDRUbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "embedding_vector_length = 32\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E95DpGo8itw6",
        "colab_type": "text"
      },
      "source": [
        "### Build an RNN using LSTM\n",
        "\n",
        "We want the model to learn the association of the sentiment with the sequence of words. And hence RNN. We will specifically use LSTM for it's ability to 'remember' long term dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XnpOkfQRUbL",
        "colab_type": "code",
        "outputId": "22261ea2-bca7-4a90-a5e2-40d76767515f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 21:56:17.210970 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0823 21:56:17.226783 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0823 21:56:17.240900 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0823 21:56:17.248678 140503148017536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0823 21:56:17.466277 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0823 21:56:17.485640 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0823 21:56:17.490890 140503148017536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 32)           320000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 300, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               8448      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 337,025\n",
            "Trainable params: 337,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRvJWLwRUbN",
        "colab_type": "code",
        "outputId": "e074849a-a31f-4c05-c7fd-a94208e8a1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epochs,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 147s 7ms/step - loss: 0.4340 - acc: 0.7898 - val_loss: 0.3061 - val_acc: 0.8746\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 145s 7ms/step - loss: 0.2363 - acc: 0.9080 - val_loss: 0.3001 - val_acc: 0.8782\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 145s 7ms/step - loss: 0.1722 - acc: 0.9369 - val_loss: 0.3235 - val_acc: 0.8798\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 144s 7ms/step - loss: 0.1313 - acc: 0.9524 - val_loss: 0.3409 - val_acc: 0.8810\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 145s 7ms/step - loss: 0.0908 - acc: 0.9684 - val_loss: 0.4523 - val_acc: 0.8660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc92530fba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31rHMhb1fy7o",
        "colab_type": "code",
        "outputId": "ee70f3d0-2de3-4349-ea85-fa557c1c405c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "log_loss, acc  = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Log loss:', log_loss)\n",
        "print('Test set accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log loss: 0.46912706779241564\n",
            "Test set accuracy: 0.85868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3TXpJo8RUbP",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjIYYvBRjoIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "TEST_INPUT_INDEX = 3\n",
        "\n",
        "def print_layer_outputs(model, test_input_index=TEST_INPUT_INDEX):\n",
        "    input_ = model.input                                        # input placeholder\n",
        "    outputs = [layer.output for layer in model.layers]     # all layer outputs\n",
        "    func = K.function([input_, K.learning_phase()], outputs )   # evaluation function\n",
        "\n",
        "    # Testing\n",
        "    test = [x_test[test_input_index]]\n",
        "    layer_outs = func([test, 1.])\n",
        "\n",
        "    for i, layer_out in enumerate(layer_outs):\n",
        "        print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "        print(layer_out)\n",
        "        print()\n",
        "\n",
        "    print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[test_input_index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL_nSoNURUbQ",
        "colab_type": "code",
        "outputId": "62a8f1f4-1cdf-49c0-8fc6-879fd5270484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print_layer_outputs(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_1/embedding_lookup/Identity:0) : (1, 300, 32)\n",
            "[[[ 0.04913902  0.03158562  0.01610495 ...  0.03528945  0.00819031\n",
            "    0.0286692 ]\n",
            "  [ 0.04913902  0.03158562  0.01610495 ...  0.03528945  0.00819031\n",
            "    0.0286692 ]\n",
            "  [ 0.04913902  0.03158562  0.01610495 ...  0.03528945  0.00819031\n",
            "    0.0286692 ]\n",
            "  ...\n",
            "  [-0.00644389 -0.03355484  0.04389771 ...  0.05145062  0.0228942\n",
            "    0.01330343]\n",
            "  [-0.04806447 -0.01907362 -0.00738712 ... -0.02627657 -0.00144542\n",
            "    0.02984534]\n",
            "  [-0.00417691  0.01354503 -0.01274874 ...  0.01298238  0.02059083\n",
            "    0.05622735]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 2 (dropout_1/cond/Merge:0) : (1, 300, 32)\n",
            "[[[ 0.          0.03948202  0.02013119 ...  0.          0.01023789\n",
            "    0.0358365 ]\n",
            "  [ 0.06142377  0.03948202  0.02013119 ...  0.04411181  0.01023789\n",
            "    0.0358365 ]\n",
            "  [ 0.06142377  0.03948202  0.02013119 ...  0.04411181  0.01023789\n",
            "    0.0358365 ]\n",
            "  ...\n",
            "  [-0.00805487 -0.          0.05487214 ...  0.06431327  0.02861775\n",
            "    0.        ]\n",
            "  [-0.         -0.02384203 -0.00923389 ... -0.03284571 -0.\n",
            "    0.03730667]\n",
            "  [-0.00522113  0.01693128 -0.01593592 ...  0.01622798  0.02573853\n",
            "    0.07028418]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 3 (lstm_1/TensorArrayReadV3:0) : (1, 32)\n",
            "[[ 0.009691    0.02762916 -0.01948995  0.03336527  0.11380878  0.10530815\n",
            "   0.06349856 -0.03902402 -0.01019245 -0.05868134  0.10462709 -0.00024508\n",
            "  -0.03405294  0.00836264  0.07012717 -0.08202592  0.02122849  0.04744293\n",
            "   0.02894847  0.08726601  0.064329    0.08396143 -0.09401151  0.01767242\n",
            "  -0.03083876 -0.09453642 -0.10012142 -0.01229374  0.0597793  -0.08712102\n",
            "  -0.03738479  0.01217221]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 4 (dense_1/Relu:0) : (1, 256)\n",
            "[[0.         0.         0.         0.03303869 0.         0.07722859\n",
            "  0.         0.08243552 0.16997093 0.11454134 0.09197252 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.01476204 0.01460306 0.         0.09321716 0.05617355\n",
            "  0.         0.         0.         0.         0.12098965 0.\n",
            "  0.         0.         0.11658493 0.1239686  0.13106652 0.\n",
            "  0.11699107 0.         0.14493002 0.09553611 0.         0.\n",
            "  0.         0.02103188 0.         0.11007605 0.05887726 0.04200754\n",
            "  0.05534733 0.         0.         0.05218249 0.06885815 0.\n",
            "  0.         0.02979444 0.         0.1278117  0.00531149 0.09066176\n",
            "  0.         0.         0.         0.05884594 0.07572374 0.\n",
            "  0.         0.07091905 0.06443176 0.08005187 0.08140981 0.\n",
            "  0.11433937 0.         0.11722338 0.         0.         0.\n",
            "  0.         0.0672176  0.         0.01961209 0.         0.03178252\n",
            "  0.         0.         0.         0.10165212 0.03751056 0.07059222\n",
            "  0.         0.09824022 0.04634975 0.         0.         0.\n",
            "  0.08193873 0.0993135  0.         0.         0.03060849 0.07981749\n",
            "  0.05820972 0.10601622 0.03558653 0.0312495  0.         0.09423342\n",
            "  0.09464519 0.         0.03323585 0.         0.00035868 0.\n",
            "  0.         0.08354732 0.06790269 0.04046945 0.02273144 0.10411353\n",
            "  0.09196135 0.         0.         0.         0.11862127 0.\n",
            "  0.         0.         0.04846306 0.08226043 0.         0.\n",
            "  0.02654807 0.07878796 0.03164461 0.09621435 0.         0.02558514\n",
            "  0.06580467 0.         0.         0.         0.11482903 0.03312274\n",
            "  0.05211678 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06159195 0.10210258 0.05476211 0.08614109\n",
            "  0.04152262 0.02911976 0.         0.         0.         0.00084159\n",
            "  0.         0.         0.         0.07243    0.08887593 0.\n",
            "  0.         0.         0.1008288  0.         0.         0.\n",
            "  0.         0.05781583 0.         0.04249446 0.         0.06748663\n",
            "  0.09600881 0.1259932  0.03410225 0.         0.06719919 0.\n",
            "  0.09625401 0.02897257 0.         0.         0.06644349 0.00032071\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.10447536 0.06233244 0.\n",
            "  0.04626821 0.04755988 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0061127  0.01042894 0.\n",
            "  0.07936326 0.09986997 0.         0.10184956 0.         0.07509913\n",
            "  0.         0.12187056 0.09567071 0.         0.09634437 0.\n",
            "  0.07843108 0.         0.07375815 0.03525718 0.07606364 0.00569965\n",
            "  0.06321738 0.         0.         0.11366631 0.08867036 0.01009551\n",
            "  0.0859979  0.         0.01751263 0.06662703 0.10676654 0.\n",
            "  0.01399103 0.08421768 0.03428993 0.06434994 0.         0.02789957\n",
            "  0.14266726 0.         0.08083636 0.01383088]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 5 (dropout_2/cond/Merge:0) : (1, 256)\n",
            "[[0.         0.         0.         0.         0.         0.09653574\n",
            "  0.         0.1030444  0.21246366 0.14317667 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.01845255 0.01825383 0.         0.11652146 0.07021694\n",
            "  0.         0.         0.         0.         0.15123707 0.\n",
            "  0.         0.         0.14573117 0.15496075 0.         0.\n",
            "  0.14623883 0.         0.18116252 0.11942014 0.         0.\n",
            "  0.         0.02628985 0.         0.13759507 0.07359658 0.05250942\n",
            "  0.06918417 0.         0.         0.         0.08607268 0.\n",
            "  0.         0.         0.         0.         0.00663937 0.11332721\n",
            "  0.         0.         0.         0.07355743 0.09465467 0.\n",
            "  0.         0.08864881 0.0805397  0.10006484 0.         0.\n",
            "  0.14292422 0.         0.14652923 0.         0.         0.\n",
            "  0.         0.084022   0.         0.         0.         0.03972815\n",
            "  0.         0.         0.         0.12706515 0.         0.08824028\n",
            "  0.         0.12280028 0.         0.         0.         0.\n",
            "  0.10242341 0.12414188 0.         0.         0.03826061 0.09977186\n",
            "  0.07276215 0.13252027 0.04448316 0.         0.         0.11779177\n",
            "  0.         0.         0.         0.         0.00044835 0.\n",
            "  0.         0.10443415 0.08487836 0.0505868  0.0284143  0.13014191\n",
            "  0.         0.         0.         0.         0.1482766  0.\n",
            "  0.         0.         0.06057882 0.10282554 0.         0.\n",
            "  0.03318509 0.09848495 0.03955576 0.         0.         0.03198143\n",
            "  0.08225583 0.         0.         0.         0.         0.04140343\n",
            "  0.06514598 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.07698993 0.12762822 0.06845263 0.10767637\n",
            "  0.05190327 0.0363997  0.         0.         0.         0.00105199\n",
            "  0.         0.         0.         0.0905375  0.11109491 0.\n",
            "  0.         0.         0.12603599 0.         0.         0.\n",
            "  0.         0.07226978 0.         0.05311807 0.         0.\n",
            "  0.12001101 0.1574915  0.04262782 0.         0.08399898 0.\n",
            "  0.1203175  0.03621571 0.         0.         0.08305436 0.00040089\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.13059421 0.07791556 0.\n",
            "  0.05783526 0.05944985 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.01303618 0.\n",
            "  0.         0.12483747 0.         0.12731196 0.         0.\n",
            "  0.         0.1523382  0.11958839 0.         0.12043045 0.\n",
            "  0.09803884 0.         0.09219769 0.04407147 0.09507955 0.00712456\n",
            "  0.07902172 0.         0.         0.14208288 0.         0.01261939\n",
            "  0.10749738 0.         0.02189079 0.08328378 0.13345817 0.\n",
            "  0.01748879 0.10527209 0.04286242 0.08043743 0.         0.03487447\n",
            "  0.17833409 0.         0.10104544 0.0172886 ]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 6 (dense_2/Sigmoid:0) : (1, 1)\n",
            "[[0.26573306]]\n",
            "\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etjCQ7mXkf7y",
        "colab_type": "text"
      },
      "source": [
        "### Build a vanilla NN (Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9He7N2NjRUbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Flatten\n",
        "\n",
        "vanilla_model = Sequential()\n",
        "vanilla_model.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
        "vanilla_model.add(Flatten())\n",
        "vanilla_model.add(Dense(250, activation='relu'))\n",
        "vanilla_model.add(Dense(1, activation='sigmoid'))\n",
        "vanilla_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfPL-YMxkyoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import callbacks\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
        "                                         patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBG76ITvRUbU",
        "colab_type": "code",
        "outputId": "5fe61b00-86f5-46f1-e532-ec74c526168b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "vanilla_model.fit(x_train, y_train, batch_size=64, epochs=5,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "loss, acc = vanilla_model.evaluate(x_test, y_test, batch_size=64)\n",
        "\n",
        "print('Test loss (LOWER is better)      :', loss)\n",
        "print('Test accuracy (HIGHER is better) :', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 6s 224us/step - loss: 0.3927 - acc: 0.8075 - val_loss: 0.3173 - val_acc: 0.8625\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 0.0652 - acc: 0.9783 - val_loss: 0.4335 - val_acc: 0.8538\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.5357 - val_acc: 0.8636\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 5s 199us/step - loss: 4.4188e-04 - acc: 1.0000 - val_loss: 0.5919 - val_acc: 0.8649\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 5s 199us/step - loss: 1.1405e-04 - acc: 1.0000 - val_loss: 0.6197 - val_acc: 0.8647\n",
            "25000/25000 [==============================] - 1s 24us/step\n",
            "Test loss (LOWER is better)      : 0.6196695337963104\n",
            "Test accuracy (HIGHER is better) : 0.8646799999809265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1t70mElSC5",
        "colab_type": "code",
        "outputId": "ca17c362-b081-4f01-8c1e-f0887eb0e2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_ = vanilla_model.input                                # input placeholder\n",
        "outputs = [layer.output for layer in vanilla_model.layers]  # all layer outputs\n",
        "func = K.function([input_, K.learning_phase()], outputs )   # evaluation function\n",
        "\n",
        "# Testing\n",
        "test = [x_test[TEST_INPUT_INDEX]]\n",
        "layer_outs = func([test, 1.])\n",
        "\n",
        "for i, layer_out in enumerate(layer_outs):\n",
        "    print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "    print(layer_out)\n",
        "    print()\n",
        "\n",
        "print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[TEST_INPUT_INDEX]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_3/embedding_lookup/Identity:0) : (1, 300, 128)\n",
            "[[[-0.00587067 -0.00898845  0.01672963 ...  0.00587083 -0.00394755\n",
            "   -0.00090874]\n",
            "  [-0.00587067 -0.00898845  0.01672963 ...  0.00587083 -0.00394755\n",
            "   -0.00090874]\n",
            "  [-0.00587067 -0.00898845  0.01672963 ...  0.00587083 -0.00394755\n",
            "   -0.00090874]\n",
            "  ...\n",
            "  [ 0.00952988 -0.05441776 -0.02427949 ...  0.03279497  0.03097179\n",
            "   -0.02037241]\n",
            "  [-0.01376792  0.05415057 -0.02318494 ... -0.05287367 -0.00226563\n",
            "    0.00585455]\n",
            "  [-0.09664261 -0.01673361  0.05815674 ...  0.04789312 -0.04545556\n",
            "    0.00793123]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 2 (flatten_1/Reshape:0) : (1, 38400)\n",
            "[[-0.00587067 -0.00898845  0.01672963 ...  0.04789312 -0.04545556\n",
            "   0.00793123]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 3 (dense_3/Relu:0) : (1, 250)\n",
            "[[0.31334057 0.6927254  0.         0.         0.59329957 0.48882878\n",
            "  0.15760882 0.         0.08844819 0.11062352 0.3523024  0.\n",
            "  0.6026896  0.41464713 0.42211902 0.7399328  0.45864972 0.\n",
            "  0.5103243  0.         0.2700956  0.3898157  0.         0.\n",
            "  0.49838132 0.14761797 0.         0.33849096 0.44736254 0.3679057\n",
            "  0.         0.4042049  0.06789493 0.25787333 0.32047033 0.\n",
            "  0.42223087 0.         0.         0.12614916 0.         0.47770754\n",
            "  0.7439741  0.49089062 0.         0.61169934 0.0760382  0.12157373\n",
            "  0.01489906 0.13752079 0.32398957 0.         0.         0.\n",
            "  0.0292334  0.         0.         0.         0.45460287 0.\n",
            "  0.5259033  0.70405024 0.         0.         0.7739188  0.\n",
            "  0.3933633  0.         0.48297557 0.72596663 0.         0.\n",
            "  0.20474008 0.7193148  0.39119074 0.         0.         0.53301436\n",
            "  0.4035971  0.01053113 0.41571766 0.643523   0.         0.40053993\n",
            "  0.         0.41368684 0.         0.         0.         0.6369208\n",
            "  0.3922859  0.         0.76832086 0.         0.1525632  0.\n",
            "  0.5434877  0.22918311 0.02760698 0.         0.00619997 0.32948872\n",
            "  0.         0.5994678  0.25842506 0.43801948 0.2724735  0.619166\n",
            "  0.25398195 0.6541242  0.32896456 0.600846   0.42729443 0.5559952\n",
            "  0.         0.07780965 0.         0.66046876 0.32381663 0.\n",
            "  0.22289717 0.         0.6786738  0.28133294 0.5096429  0.61454433\n",
            "  0.         0.42863455 0.35702947 0.45679167 0.49191004 0.32437477\n",
            "  0.52048707 0.30638105 0.19173196 0.         0.12129783 0.09316193\n",
            "  0.         0.         0.21977806 0.52754515 0.         0.\n",
            "  0.34749785 0.         0.46689007 0.15388452 0.5709405  0.52195954\n",
            "  0.06788675 0.53393453 0.21654868 0.         0.00410527 0.42989776\n",
            "  0.5563248  0.5432449  0.27994004 0.27145243 0.50727487 0.4266225\n",
            "  0.04254073 0.34491083 0.18688951 0.6535299  0.21358034 0.\n",
            "  0.5004108  0.62970966 0.41416255 0.07537014 0.         0.54904366\n",
            "  0.54266965 0.44386747 0.48600474 0.         0.         0.66547555\n",
            "  0.611944   0.62202287 0.         0.22802876 0.58433497 0.\n",
            "  0.6590668  0.5537015  0.50248617 0.58986473 0.37729976 0.40949157\n",
            "  0.45761266 0.         0.         0.4496     0.47976226 0.32806948\n",
            "  0.         0.31197622 0.         0.         0.7334581  0.46152306\n",
            "  0.         0.         0.         0.         0.         0.24951419\n",
            "  0.         0.11895387 0.         0.5432593  0.3959608  0.\n",
            "  0.34712666 0.         0.38779128 0.47681302 0.4957009  0.\n",
            "  0.44465402 0.         0.6339191  0.12312477 0.49023104 0.54068345\n",
            "  0.6738299  0.4849327  0.4182     0.4432521  0.         0.2579225\n",
            "  0.         0.689974   0.0063997  0.         0.         0.5599252\n",
            "  0.         0.47955033 0.33430406 0.         0.         0.4080625\n",
            "  0.49734017 0.47429866 0.5796015  0.        ]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 4 (dense_4/Sigmoid:0) : (1, 1)\n",
            "[[0.1988847]]\n",
            "\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02IxM_QYlbzC",
        "colab_type": "text"
      },
      "source": [
        "### Build a CNN to learn from the structure of paragraphs/sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wrWRriAla97",
        "colab_type": "code",
        "outputId": "c72951a2-e5db-4ac8-81d7-5a18e193e631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "# create the model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(250, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 22:26:44.794390 140503148017536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB3zgbt8lihb",
        "colab_type": "code",
        "outputId": "90d997ba-47df-4d8f-eef7-a2d9336130a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "cnn_model.fit(x_train, y_train, batch_size=64, epochs=5,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "loss, acc = cnn_model.evaluate(x_test, y_test, batch_size=64)\n",
        "\n",
        "print('Test loss (LOWER is better)      :', loss)\n",
        "print('Test accuracy (HIGHER is better) :', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 4s 148us/step - loss: 0.3787 - acc: 0.8109 - val_loss: 0.2706 - val_acc: 0.8875\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 3s 140us/step - loss: 0.1594 - acc: 0.9421 - val_loss: 0.2938 - val_acc: 0.8837\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 4s 140us/step - loss: 0.0549 - acc: 0.9833 - val_loss: 0.3964 - val_acc: 0.8734\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 4s 143us/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.5443 - val_acc: 0.8773\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 4s 141us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.6549 - val_acc: 0.8739\n",
            "25000/25000 [==============================] - 1s 32us/step\n",
            "Test loss (LOWER is better)      : 0.654878829908371\n",
            "Test accuracy (HIGHER is better) : 0.8738800000381469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svdXveNrljXS",
        "colab_type": "code",
        "outputId": "3e2c7978-f933-4fed-8ec9-7818f498bae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_ = cnn_model.input                                # input placeholder\n",
        "outputs = [layer.output for layer in cnn_model.layers]  # all layer outputs\n",
        "func = K.function([input_, K.learning_phase()], outputs )   # evaluation function\n",
        "\n",
        "# Testing\n",
        "test = [x_test[TEST_INPUT_INDEX]]\n",
        "layer_outs = func([test, 1.])\n",
        "\n",
        "for i, layer_out in enumerate(layer_outs):\n",
        "    print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "    print(layer_out)\n",
        "    print()\n",
        "\n",
        "print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[TEST_INPUT_INDEX]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_5/embedding_lookup/Identity:0) : (1, 300, 128)\n",
            "[[[-0.02950282 -0.04437667 -0.00454681 ... -0.01479043 -0.0080799\n",
            "    0.02695233]\n",
            "  [-0.02950282 -0.04437667 -0.00454681 ... -0.01479043 -0.0080799\n",
            "    0.02695233]\n",
            "  [-0.02950282 -0.04437667 -0.00454681 ... -0.01479043 -0.0080799\n",
            "    0.02695233]\n",
            "  ...\n",
            "  [-0.01328565 -0.01004876 -0.02366997 ... -0.03611348  0.08023344\n",
            "    0.02460422]\n",
            "  [ 0.03224817  0.05176777  0.04430321 ... -0.01087128  0.05936193\n",
            "    0.01831979]\n",
            "  [-0.06839077 -0.03283393  0.0372564  ...  0.01095146 -0.04636538\n",
            "   -0.09619831]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 2 (conv1d_1/Relu:0) : (1, 300, 64)\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.         0.05098206 0.         ... 0.         0.15402974 0.        ]\n",
            "  [0.02540092 0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.01528286 0.14629602 0.         ... 0.08346704 0.06966806 0.        ]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 3 (max_pooling1d_1/Squeeze:0) : (1, 150, 64)\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.0485356  0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.05098206 0.         ... 0.         0.15402974 0.        ]\n",
            "  [0.02540092 0.14629602 0.         ... 0.08346704 0.06966806 0.        ]]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 4 (flatten_2/Reshape:0) : (1, 9600)\n",
            "[[0.         0.         0.         ... 0.08346704 0.06966806 0.        ]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 5 (dense_5/Relu:0) : (1, 250)\n",
            "[[1.4593701  0.         0.         0.         0.         1.2871811\n",
            "  0.         0.         0.         0.         0.         1.1751763\n",
            "  0.         0.         0.         0.         0.         1.6484092\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.6865553  1.1031793  0.         0.\n",
            "  0.         0.         0.         1.8332139  1.1670086  0.8741625\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.8711603  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.8004235  0.         0.         0.\n",
            "  0.         0.         0.         0.         2.0738497  1.119371\n",
            "  1.5136173  0.         0.         1.7285708  0.9082286  1.391037\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.3854575  0.\n",
            "  0.29107904 0.         0.         0.         1.5732313  2.3132765\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.4435399  0.96272826 0.         0.         0.\n",
            "  0.         0.         0.         0.9865707  1.3669742  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.6293283  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.8078281\n",
            "  0.         0.         0.         0.         0.8209545  0.\n",
            "  1.8024632  0.         0.         0.         0.         0.82568735\n",
            "  0.         0.81109285 0.         0.         0.         0.8916639\n",
            "  0.         0.         1.0240008  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.714225   0.         0.96959877\n",
            "  0.         0.         0.         0.7095029  0.         0.\n",
            "  0.         0.7728009  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.4276395  0.         0.         0.\n",
            "  0.         0.         0.         0.52454585 0.         0.9587845\n",
            "  1.2841791  0.         0.         0.         0.         1.4747256\n",
            "  0.         1.3819034  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.3269069\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.9411709  0.\n",
            "  0.         0.         0.         0.        ]]\n",
            "\n",
            "OUTPUT SHAPE for Layer 6 (dense_6/Sigmoid:0) : (1, 1)\n",
            "[[0.2582979]]\n",
            "\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR6BWL-Rl7NS",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZbXc1_sl_U7",
        "colab_type": "code",
        "outputId": "08543351-4c56-40f1-d258-1e700078a684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Turn the data into strings of integers, because that's how the\n",
        "# CountVectorizer likes it.\n",
        "x_train_s = [' '.join(map(str, row)) for row in x_train]\n",
        "x_test_s = [' '.join(map(str, row)) for row in x_test]\n",
        "\n",
        "# We'll just use the default values.\n",
        "pipeline = Pipeline([('counter', TfidfVectorizer()), \n",
        "                     ('classifier', LogisticRegression())])\n",
        "pipeline.fit(x_train_s, y_train)\n",
        "\n",
        "y_sklearn = pipeline.predict(x_test_s)\n",
        "y_proba_sklearn = pipeline.predict_proba(x_test_s)\n",
        "\n",
        "print('Test loss     : {}'.format(log_loss(y_test, y_proba_sklearn)))\n",
        "print('Test accuracy : {}'.format(accuracy_score(y_test, y_sklearn)))\n",
        "print('Test f1 score : {}'.format(f1_score(y_test, y_sklearn)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss     : 0.31094379445503795\n",
            "Test accuracy : 0.88408\n",
            "Test f1 score : 0.8843575418994414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RyLGgCmJN0",
        "colab_type": "text"
      },
      "source": [
        "## Observations\n",
        "\n",
        "### LSTM\n",
        "\n",
        "*   Test loss : **0.46912706779241564**\n",
        "*   Test accuracy : **0.85868**\n",
        "*   Epochs : 5\n",
        "*   Overfitted : Yes\n",
        "\n",
        "\n",
        "### Vanilla NN/MLP\n",
        "*   Test loss (LOWER is better) : **0.6196695337963104**\n",
        "*   Test accuracy (HIGHER is better) : **0.8646799999809265**\n",
        "*   Epochs : 5\n",
        "*   Overfitted : Yes\n",
        "\n",
        "### CNN\n",
        "*   Test loss (LOWER is better) : **0.654878829908371**\n",
        "*   Test accuracy (HIGHER is better) : **0.8738800000381469**\n",
        "*   Epochs : 5\n",
        "*   Overfitted : Yes\n",
        "\n",
        "\n",
        "### Logistic Regression\n",
        "*   Test loss     : **0.31094379445503795**\n",
        "*   Test accuracy : **0.88408**\n",
        "*   Test f1 score : **0.8843575418994414**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOwDEMMBoKvc",
        "colab_type": "text"
      },
      "source": [
        "## Final Note\n",
        "\n",
        "Among the Neural Network models, the LSTM based RNN was the slowest to overfit. The MLP and the CNN quickly overfitted. Even though the LSTM based RNN has the least accuracy score, it is likely to generalize better IF the dataset is bigger because of it ability to learn complex relations over larger contexts. As such with the smaller dataset provided to these models, a simple Logistic Regression did better!"
      ]
    }
  ]
}